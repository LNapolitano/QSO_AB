{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7becfa57-0f9b-481d-94be-f0a4762145ae",
   "metadata": {},
   "source": [
    "Code designed to write catalogs of absorbers from .h5py files stored within each healpix directory. Then individual healpix catalogs are merged into a single release based file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43510c48-092a-43f9-8799-f250c179862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "import desispec.io\n",
    "import fitsio\n",
    "from desispec.coaddition import coadd_cameras\n",
    "from desimodel.footprint import radec2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "first_line_wave=2796.3543\n",
    "second_line_wave=2803.5315\n",
    "\n",
    "ndim=7\n",
    "#file path setup for MgII chains\n",
    "base_fp='/global/cscratch1/sd/lucasnap/MgII_Abs_Chains'\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9f49cf-30a3-4f15-9ca7-bb97318b44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elliptical Selection Params\n",
    "h,k,a,b,A=[1.2,1.2,1,1.2,np.pi/3]\n",
    "\n",
    "#Purity cuts developed from visual inspection. Accepts MgII_catalog and returns MgII_catalog following cuts\n",
    "def purity_cuts(MgII_cat,pre_ellip_cut=False,PIA_cut=True):\n",
    "    \n",
    "    if(PIA_cut):\n",
    "        print('Removing {} physically impossible absorption systems'.format(np.sum((MgII_cat['Z_MGII']>0.05+MgII_cat['Z_QSO']))))\n",
    "        MgII_cat=MgII_cat[~(MgII_cat['Z_MGII']>0.05+MgII_cat['Z_QSO'])]\n",
    "    #record inital size for later comparison\n",
    "    int_size=len(MgII_cat)\n",
    "    \n",
    "    #first cut positive amplitudes\n",
    "    MgII_cat=MgII_cat[MgII_cat['AMP_2796']<0]\n",
    "    MgII_cat=MgII_cat[MgII_cat['AMP_2803']<0]\n",
    "    \n",
    "    #Print results and save new catalog size\n",
    "    print('Result of Positive Amplitude Cuts: {} systems removed'.format(int_size-len(MgII_cat)))\n",
    "    sec_size=len(MgII_cat)\n",
    "\n",
    "    \n",
    "    #Print results and save new catalog size\n",
    "    #print('Result of Large Negative Spike Cuts: {} systems removed'.format(sec_size-len(MgII_cat)))\n",
    "    ter_size=len(MgII_cat)\n",
    "    \n",
    "    #Finally perform elliptical selection\n",
    "    xt = MgII_cat['AMP_2796']/MgII_cat['AMP_2803']\n",
    "    yt = MgII_cat['STDDEV_2796']/MgII_cat['STDDEV_2803']\n",
    "\n",
    "    #gives number of systems post initial cuts that lie outside selection\n",
    "    ellipse_mask=((xt-h)*np.cos(A)+(yt-k)*np.sin(A))**2/a**2+((xt-h)*np.sin(A)-(yt-k)*np.cos(A))**2/b**2>1\n",
    "\n",
    "    #resulting #systems given by \n",
    "    print('Result of Elliptical Cuts: {} systems removed'.format(np.sum(ellipse_mask)))\n",
    "    print('Result of Purity Cuts: {} systems removed of initial {}\\n'.format(int_size-ter_size+np.sum(ellipse_mask),int_size))\n",
    "    \n",
    "    #return catalog results before elliptical cuts for purposes of plotting and actual number of systems post-cut for numbers\n",
    "    if(pre_ellip_cut):\n",
    "        return(MgII_cat,int_size-(int_size-ter_size+np.sum(ellipse_mask)))\n",
    "    else:\n",
    "        return(MgII_cat[~ellipse_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c89d960-193a-41a8-9f62-9d2ac039e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying reduction\n",
    "redux='guadalupe'\n",
    "\n",
    "#file path setup for MgII chains\n",
    "chains_fp=os.path.join(base_fp,redux)\n",
    "\n",
    "#file path setup for reduction directories\n",
    "reduction_base_dir='/global/cfs/cdirs/desi/spectro/redux/{}/'.format(redux)\n",
    "summary_cat_dir=os.path.join(reduction_base_dir,'zcatalog')\n",
    "\n",
    "#reading all survey zcats and forming a dictionary to easily access when needed\n",
    "if(redux == 'fuji'):\n",
    "    sv1_zcat=fitsio.read('{}/zpix-{}-dark.fits'.format(summary_cat_dir,'sv1'),'ZCATALOG')\n",
    "    sv2_zcat=fitsio.read('{}/zpix-{}-dark.fits'.format(summary_cat_dir,'sv2'),'ZCATALOG')\n",
    "    sv3_zcat=fitsio.read('{}/zpix-{}-dark.fits'.format(summary_cat_dir,'sv3'),'ZCATALOG')\n",
    "    \n",
    "    zcat_dic={'sv1':sv1_zcat,'sv2':sv2_zcat,'sv3':sv3_zcat}\n",
    "    \n",
    "if(redux == 'guadalupe'):\n",
    "    main_zcat=fitsio.read('{}/zpix-{}-dark.fits'.format(summary_cat_dir,'main'),'ZCATALOG')\n",
    "    \n",
    "    zcat_dic={'main':main_zcat}\n",
    "\n",
    "#reading QSOcat\n",
    "QSOcat_fp='/global/cfs/cdirs/desi/users/edmondc/QSO_catalog/{}/QSO_cat_{}_healpix.fits'.format(redux,redux)\n",
    "QSOcat=fitsio.read(QSOcat_fp,'QSO_CAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ba7c62-5987-436f-9f6e-a20fe36bf530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3790\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#lists to store the healpix directories with/without written .fits MgII_catalogs\n",
    "hp_complete=[]\n",
    "hp_incomplete=[]\n",
    "\n",
    "for short_hp in os.listdir(chains_fp):\n",
    "        \n",
    "        short_fp=os.path.join(chains_fp,short_hp)\n",
    "        \n",
    "        #check if filepath is a directory\n",
    "        #if not move to next entry\n",
    "        if not os.path.isdir(short_fp):\n",
    "            continue\n",
    "\n",
    "        for healpix in os.listdir(short_fp):\n",
    "            \n",
    "            #output prep, removing file if it exists to avoid appending rather than overwriting\n",
    "            out_fn='{}/MgII-Absorbers-{}.fits'.format(os.path.join(chains_fp,short_hp,healpix),healpix)\n",
    "            \n",
    "\n",
    "            #check if out-file exists and if it does remove it\n",
    "            if os.path.exists(out_fn):\n",
    "                hp_complete.append(healpix)\n",
    "                continue\n",
    "            else:\n",
    "                hp_incomplete.append(healpix)\n",
    "                \n",
    "print(len(hp_complete))\n",
    "print(len(hp_incomplete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b8a5c38-e1e5-48f7-9691-44f2a5bb6ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify if existing MgII_catalogs should be replaced\n",
    "replace_files=False\n",
    "\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "med_filt_size=99\n",
    "\n",
    "first_line_wave=2796.3543\n",
    "second_line_wave=2803.5315\n",
    "center=(second_line_wave+first_line_wave)/2\n",
    "\n",
    "if(redux=='fuji'):\n",
    "    possible_srv=['sv1','sv2','sv3']\n",
    "elif(redux=='guadalupe'):\n",
    "    possible_srv=['main']\n",
    "\n",
    "for healpix in hp_incomplete:\n",
    "        \n",
    "        #healpixel abbreviation\n",
    "        short_hp=healpix[:-2]\n",
    "\n",
    "        #clear outputs for tidiness\n",
    "        clear_output()\n",
    "\n",
    "        #output prep, removing file if it exists to avoid appending rather than overwriting\n",
    "        out_fn='{}/MgII-Absorbers-{}.fits'.format(os.path.join(chains_fp,short_hp,healpix),healpix)\n",
    "\n",
    "        #go grab spectra file\n",
    "        coadd_dic={}\n",
    "        for survey in possible_srv:\n",
    "            hp_dir=os.path.join(reduction_base_dir,'healpix',survey,'dark',healpix[0:-2],healpix)\n",
    "            specfile=os.path.join(hp_dir,'coadd-{}-dark-{}.fits'.format(survey,healpix))\n",
    "\n",
    "            if os.path.exists(specfile):\n",
    "                specobj = desispec.io.read_spectra(specfile)\n",
    "\n",
    "                coadd_dic[survey] = specobj\n",
    "\n",
    "\n",
    "        #Stupid dumb way to hold lists of values\n",
    "        TARGETIDs=[]\n",
    "        RAs=[]\n",
    "        DECs=[]\n",
    "        Surveys=[]\n",
    "        ZWarns=[]\n",
    "        TSNR_QSOs=[]\n",
    "        TSNR_LYAs=[]\n",
    "        TSNR_LRGs=[]\n",
    "        z_QSOs=[]\n",
    "\n",
    "        EW_2796s_cen=[]; EW_2796s_low=[]; EW_2796s_high=[]\n",
    "        EW_2803s_cen=[]; EW_2803s_low=[]; EW_2803s_high=[]\n",
    "\n",
    "        z_MgIIs_cen=[]; z_MgIIs_low=[]; z_MgIIs_high=[]\n",
    "        Amp1s_cen=[]; Amp1s_low=[]; Amp1s_high=[]\n",
    "        Amp2s_cen=[]; Amp2s_low=[]; Amp2s_high=[]\n",
    "        StdDev1s_cen=[]; StdDev1s_low=[]; StdDev1s_high=[]\n",
    "        StdDev2s_cen=[]; StdDev2s_low=[]; StdDev2s_high=[]\n",
    "        Slopes_cen=[]; Slopes_low=[]; Slopes_high=[];\n",
    "        Intercepts_cen=[]; Intercepts_low=[]; Intercepts_high=[]\n",
    "\n",
    "        LineSNRmins=[]\n",
    "        LineSNRmaxs=[]\n",
    "\n",
    "\n",
    "\n",
    "        h5_fn='MgII-Abs-Chains-{}.hdf5'.format(healpix)\n",
    "        h5_fp=os.path.join(chains_fp,short_hp,healpix,h5_fn)\n",
    "        if(os.path.exists(h5_fp)):\n",
    "            h5_file=h5py.File(h5_fp,'r')\n",
    "        else:\n",
    "            print('Broken file: {}'.format(h5_fp))\n",
    "            continue\n",
    "\n",
    "        #print(h5_file.keys())\n",
    "        for key in h5_file.keys():\n",
    "\n",
    "            TARGETID,survey,hp,z_MgII,z_QSO,snr1,snr2=key.split('_')\n",
    "\n",
    "            #excluding special survey at the moment\n",
    "            if(survey=='special'):\n",
    "                continue\n",
    "\n",
    "            #grab correct summary catalog and coadd file\n",
    "            coadd=coadd_dic[survey]\n",
    "            summary_cat=zcat_dic[survey]\n",
    "            #find entry in summary catalog\n",
    "            sum_cat_idx=np.where(summary_cat['TARGETID']==int(TARGETID))[0]\n",
    "            specobj_idx=np.where(coadd.target_ids() == int(TARGETID))[0]\n",
    "\n",
    "            #grab correct spectra and coadd (over cameras)\n",
    "            spec=coadd_cameras(coadd[specobj_idx])\n",
    "\n",
    "            #grab flux values\n",
    "            x_spc = spec.wave[\"brz\"]\n",
    "            y_flx = spec.flux[\"brz\"][0]\n",
    "\n",
    "            #estimate continuum using median filter\n",
    "            cont_est = medfilt(y_flx,med_filt_size)\n",
    "\n",
    "            #find correct index for center of doublet\n",
    "            index=np.abs(x_spc - (center*(1+float(z_MgII)))).argmin()\n",
    "            #estimate continuum at this center\n",
    "            continuum=float(cont_est[index])\n",
    "\n",
    "            #en[\"NEW_EW2796\"]=np.abs(en['AMP_2796'])*en['STDDEV_2796']*np.sqrt(2*np.pi)/(1.0+en['Z_MGII'])/continuum\n",
    "            #en[\"NEW_EW2803\"]=np.abs(en['AMP_2803'])*en['STDDEV_2803']*np.sqrt(2*np.pi)/(1.0+en['Z_MGII'])/continuum\n",
    "\n",
    "            #pull info from summary catalog, ra, dec, rmag\n",
    "            ra=summary_cat['TARGET_RA'][sum_cat_idx]\n",
    "            dec=summary_cat['TARGET_DEC'][sum_cat_idx]\n",
    "\n",
    "            ZWarn=summary_cat['ZWARN'][sum_cat_idx]\n",
    "\n",
    "            TSNR_QSO=summary_cat['TSNR2_QSO'][sum_cat_idx]\n",
    "            TSNR_LYA=summary_cat['TSNR2_LYA'][sum_cat_idx]\n",
    "            TSNR_LRG=summary_cat['TSNR2_LRG'][sum_cat_idx]\n",
    "\n",
    "            #RFlux=summary_cat['FLUX_R'][sum_cat_idx]\n",
    "\n",
    "            #grab datasets assocaited with key\n",
    "            dset = h5_file[key]\n",
    "            #grab datasets keys\n",
    "            dkeys=dset.keys()\n",
    "\n",
    "            #hold attributes of MgII abs. fit\n",
    "            feature_mid=np.zeros(ndim)\n",
    "            feature_low=np.zeros(ndim)\n",
    "            feature_high=np.zeros(ndim)\n",
    "\n",
    "            for i,dkey in zip(range(ndim),dkeys):\n",
    "                feature_mid[i]=np.percentile(dset[dkey],50)\n",
    "                feature_low[i]=np.percentile(dset[dkey],16)\n",
    "                feature_high[i]=np.percentile(dset[dkey],84)\n",
    "\n",
    "\n",
    "            #calculate line equivalent widths using amplitudes, widths, redshift and line intercept* (TODO: evaluate this choice?)\n",
    "            EW_2796s=np.abs(np.random.choice(dset['Amp1'],10000))*np.random.choice(dset['StdDev1'],10000)*np.sqrt(2*np.pi)\\\n",
    "            /(1+np.random.choice(dset['z'],10000))/continuum\n",
    "\n",
    "            EW_2803s=np.abs(np.random.choice(dset['Amp2'],10000))*np.random.choice(dset['StdDev2'],10000)*np.sqrt(2*np.pi)\\\n",
    "            /(1+np.random.choice(dset['z'],10000))/continuum\n",
    "\n",
    "            #append everything to the dumb arrays\n",
    "            TARGETIDs.append(int(TARGETID))\n",
    "            RAs.append(ra)\n",
    "            DECs.append(dec)\n",
    "            Surveys.append(survey)\n",
    "            ZWarns.append(ZWarn)\n",
    "            TSNR_QSOs.append(TSNR_QSO)\n",
    "            TSNR_LYAs.append(TSNR_LYA)\n",
    "            TSNR_LRGs.append(TSNR_LRG)\n",
    "            z_QSOs.append(float(z_QSO))\n",
    "\n",
    "            EW_2796s_cen.append(np.percentile(EW_2796s,50))\n",
    "            EW_2796s_low.append(np.percentile(EW_2796s,50)-np.percentile(EW_2796s,16))\n",
    "            EW_2796s_high.append(np.percentile(EW_2796s,84)-np.percentile(EW_2796s,50))\n",
    "\n",
    "            EW_2803s_cen.append(np.percentile(EW_2803s,50))\n",
    "            EW_2803s_low.append(np.percentile(EW_2803s,50)-np.percentile(EW_2803s,16))\n",
    "            EW_2803s_high.append(np.percentile(EW_2803s,84)-np.percentile(EW_2803s,50))\n",
    "\n",
    "            z_MgIIs_cen.append(feature_mid[6]); z_MgIIs_low.append(feature_mid[6]-feature_low[6]); z_MgIIs_high.append(feature_high[6]-feature_mid[6])\n",
    "            Amp1s_cen.append(feature_mid[0]); Amp1s_low.append(feature_mid[0]-feature_low[0]); Amp1s_high.append(feature_high[0]-feature_mid[0])\n",
    "            Amp2s_cen.append(feature_mid[1]); Amp2s_low.append(feature_mid[1]-feature_low[1]); Amp2s_high.append(feature_high[1]-feature_mid[1])\n",
    "            StdDev1s_cen.append(feature_mid[2]); StdDev1s_low.append(feature_mid[2]-feature_low[2]); StdDev1s_high.append(feature_high[2]-feature_mid[2])\n",
    "            StdDev2s_cen.append(feature_mid[3]); StdDev2s_low.append(feature_mid[3]-feature_low[3]); StdDev2s_high.append(feature_high[3]-feature_mid[3])\n",
    "            Intercepts_cen.append(feature_mid[4]); Intercepts_low.append(feature_mid[4]-feature_low[4]); Intercepts_high.append(feature_high[4]-feature_mid[4])\n",
    "            Slopes_cen.append(feature_mid[5]); Slopes_low.append(feature_mid[5]-feature_low[5]); Slopes_high.append(feature_high[5]-feature_mid[5])\n",
    "\n",
    "            LineSNRmins.append(float(min(snr1,snr2)))\n",
    "            LineSNRmaxs.append(float(max(snr1,snr2)))\n",
    "\n",
    "        TARGETIDs=np.asarray(TARGETIDs)\n",
    "        RAs=np.asarray(RAs)\n",
    "        DECs=np.asarray(DECs)\n",
    "        Surveys=np.asarray(Surveys)\n",
    "        ZWarns=np.asarray(ZWarns)      \n",
    "        TSNR_QSOs=np.asarray(TSNR_QSOs)\n",
    "        TSNR_LYAs=np.asarray(TSNR_LYAs)\n",
    "        TSNR_LRGs=np.asarray(TSNR_LRGs)  \n",
    "        z_QSOs=np.asarray(z_QSOs)\n",
    "\n",
    "        EW_2796s_cen=np.asarray(EW_2796s_cen); EW_2796s_low=np.asarray(EW_2796s_low); EW_2796s_high=np.asarray(EW_2796s_high)\n",
    "        EW_2803s_cen=np.asarray(EW_2803s_cen); EW_2803s_low=np.asarray(EW_2803s_low); EW_2803s_high=np.asarray(EW_2803s_high)\n",
    "\n",
    "        z_MgIIs_cen=np.asarray(z_MgIIs_cen); z_MgIIs_low=np.asarray(z_MgIIs_low); z_MgIIs_high=np.asarray(z_MgIIs_high)\n",
    "        Amp1s_cen=np.asarray(Amp1s_cen); Amp1s_low=np.asarray(Amp1s_low); Amp1s_high=np.asarray(Amp1s_high)\n",
    "        Amp2s_cen=np.asarray(Amp2s_cen); Amp2s_low=np.asarray(Amp2s_low); Amp2s_high=np.asarray(Amp2s_high)\n",
    "        StdDev1s_cen=np.asarray(StdDev1s_cen); StdDev1s_low=np.asarray(StdDev1s_low); StdDev1s_high=np.asarray(StdDev1s_high)\n",
    "        StdDev2s_cen=np.asarray(StdDev2s_cen); StdDev2s_low=np.asarray(StdDev2s_low); StdDev2s_high=np.asarray(StdDev2s_high)\n",
    "        Slopes_cen=np.asarray(Slopes_cen); Slopes_low=np.asarray(Slopes_low); Slopes_high=np.asarray(Slopes_high)\n",
    "        Intercepts_cen=np.asarray(Intercepts_cen); Intercepts_low=np.asarray(Intercepts_low); Intercepts_high=np.asarray(Intercepts_high)\n",
    "\n",
    "        LineSNRmins=np.asarray(LineSNRmins)\n",
    "        LineSNRmaxs=np.asarray(LineSNRmaxs)\n",
    "\n",
    "\n",
    "        # prepare to write out fits file\n",
    "        fits = fitsio.FITS(out_fn,'rw')\n",
    "\n",
    "        # can also be a list of ordinary arrays if you send the names\n",
    "        array_list=[TARGETIDs,RAs,DECs,Surveys,ZWarns,TSNR_QSOs,TSNR_LYAs,TSNR_LRGs,z_QSOs,EW_2796s_cen,EW_2803s_cen,\\\n",
    "                    EW_2796s_low,EW_2803s_low,\\\n",
    "                    EW_2796s_high,EW_2803s_high,z_MgIIs_cen,Amp1s_cen,Amp2s_cen,StdDev1s_cen,StdDev2s_cen,Slopes_cen,Intercepts_cen,\\\n",
    "                    z_MgIIs_low,Amp1s_low,Amp2s_low,StdDev1s_low,StdDev2s_low,Slopes_low,Intercepts_low,\\\n",
    "                    z_MgIIs_high,Amp1s_high,Amp2s_high,StdDev1s_high,StdDev2s_high,Slopes_high,Intercepts_high,\\\n",
    "                    LineSNRmins,LineSNRmaxs]\n",
    "        names=['TARGETID','RA','DEC','SURVEY','ZWARN','TSNR2_QSO','TSNR2_LYA','TSNR2_LRG','Z_QSO','EW_2796','EW_2803','EW_2796_ERR_LOW',\\\n",
    "               'EW_2803_ERR_LOW','EW_2796_ERR_HIGH',\\\n",
    "               'EW_2803_ERR_HIGH','Z_MGII','AMP_2796','AMP_2803','STDDEV_2796','STDDEV_2803','SLOPE','INTERCEPT',\\\n",
    "               'Z_MGII_ERR_LOW','AMP_2796_ERR_LOW','AMP_2803_ERR_LOW','STDDEV_2796_ERR_LOW','STDDEV_2803_ERR_LOW',\\\n",
    "               'SLOPE_ERR_LOW','INTERCEPT_ERR_LOW',\\\n",
    "               'Z_MGII_ERR_HIGH','AMP_2796_ERR_HIGH','AMP_2803_ERR_HIGH','STDDEV_2796_ERR_HIGH','STDDEV_2803_ERR_HIGH','SLOPE_ERR_HIGH',\\\n",
    "               'INTERCEPT_ERR_HIGH','LINE_SNR_MIN','LINE_SNR_MAX']\n",
    "        fits.write(array_list, names=names,clobber=True,extname='MGII_ABSORBERS')\n",
    "\n",
    "        fits.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ccfed0-c85e-401f-81c4-b812af97a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33394\n"
     ]
    }
   ],
   "source": [
    "#Merge individual healpix MgII_catalogs into master catalog\n",
    "cat_started=False\n",
    "\n",
    "for healpix in hp_complete:\n",
    "    short_hp=healpix[:-2]\n",
    "    \n",
    "    cat_fp=os.path.join(chains_fp,short_hp,healpix,'MgII-Absorbers-{}.fits'.format(healpix))\n",
    "    \n",
    "    #print(cat_fp)\n",
    "\n",
    "    try:\n",
    "        MgII_cat=fitsio.read(cat_fp)\n",
    "    except:# OSError as e:\n",
    "        #print(e)\n",
    "        continue\n",
    "    \n",
    "    if(cat_started):\n",
    "        #MgII_cat_full=MgII_cat\n",
    "        MgII_cat_full=np.concatenate((MgII_cat_full,MgII_cat),dtype=MgII_cat_full.dtype)\n",
    "    else:\n",
    "        MgII_cat_full=MgII_cat\n",
    "        cat_started=True\n",
    "\n",
    "                \n",
    "print(len(MgII_cat_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6aa926c-e45c-4a5d-a2de-9c9a424c09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read fujilupe catalog\n",
    "fujilupe=fitsio.read(\"/global/cfs/cdirs/desi/science/gqp/zcatalog_summary/zall-pix-fujilupe.fits\")\n",
    "\n",
    "#mask for only the primary observations, based on selection detailed in Fujilupe_CombineZcat.ipynb (location given at top of this notebook section)\n",
    "is_primary = fujilupe[\"ZCAT_PRIMARY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134ba6c1-243e-4537-8be5-0ca088d547c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial length: 33394\n",
      "('TARGETID', 'RA', 'DEC', 'SURVEY', 'ZWARN', 'TSNR2_QSO', 'TSNR2_LYA', 'TSNR2_LRG', 'Z_QSO', 'EW_2796', 'EW_2803', 'EW_2796_ERR_LOW', 'EW_2803_ERR_LOW', 'EW_2796_ERR_HIGH', 'EW_2803_ERR_HIGH', 'Z_MGII', 'AMP_2796', 'AMP_2803', 'STDDEV_2796', 'STDDEV_2803', 'SLOPE', 'INTERCEPT', 'Z_MGII_ERR_LOW', 'AMP_2796_ERR_LOW', 'AMP_2803_ERR_LOW', 'STDDEV_2796_ERR_LOW', 'STDDEV_2803_ERR_LOW', 'SLOPE_ERR_LOW', 'INTERCEPT_ERR_LOW', 'Z_MGII_ERR_HIGH', 'AMP_2796_ERR_HIGH', 'AMP_2803_ERR_HIGH', 'STDDEV_2796_ERR_HIGH', 'STDDEV_2803_ERR_HIGH', 'SLOPE_ERR_HIGH', 'INTERCEPT_ERR_HIGH', 'LINE_SNR_MIN', 'LINE_SNR_MAX')\n",
      "Removing 147 physically impossible absorption systems\n",
      "Result of Positive Amplitude Cuts: 513 systems removed\n",
      "Result of Elliptical Cuts: 877 systems removed\n",
      "Result of Purity Cuts: 1390 systems removed of initial 33247\n",
      "\n",
      "Post Cuts: 31857\n"
     ]
    }
   ],
   "source": [
    "#Apply purity cuts and then select the best entry for those TARGETIDs appearing in multiple surveys (fuji only)\n",
    "\n",
    "out_fn='/global/cfs/cdirs/desi/users/lucasnap/MgII-Absorbers-{}.fits'.format(redux)\n",
    "\n",
    "print('Initial length: {}'.format(len(MgII_cat_full)))\n",
    "print(MgII_cat_full.dtype.names)\n",
    "\n",
    "MgII_pure=purity_cuts(MgII_cat_full)\n",
    "print('Post Cuts: {}'.format(len(MgII_pure)))\n",
    "\n",
    "#for fuji it is necessary to select the best entry (based on TSNR2_LRG)\n",
    "if(redux =='fuji'):\n",
    "    sv1_mask=MgII_pure['SURVEY']=='sv1'\n",
    "    sv2_mask=MgII_pure['SURVEY']=='sv2'\n",
    "    sv3_mask=MgII_pure['SURVEY']=='sv3'\n",
    "\n",
    "\n",
    "    sv1_repeat_TID,sv1_repeat_idx=np.intersect1d(MgII_pure[sv1_mask]['TARGETID'],np.concatenate((MgII_pure[sv2_mask]['TARGETID'],\\\n",
    "                                                                                                 MgII_pure[sv3_mask]['TARGETID'])),return_indices=True)[:2]\n",
    "\n",
    "    sv2_repeat_TID,sv2_repeat_idx=np.intersect1d(MgII_pure[sv2_mask]['TARGETID'],np.concatenate((MgII_pure[sv1_mask]['TARGETID'],\\\n",
    "                                                                                                 MgII_pure[sv3_mask]['TARGETID'])),return_indices=True)[:2]\n",
    "\n",
    "    sv3_repeat_TID,sv3_repeat_idx=np.intersect1d(MgII_pure[sv3_mask]['TARGETID'],np.concatenate((MgII_pure[sv1_mask]['TARGETID'],\\\n",
    "                                                                                                 MgII_pure[sv2_mask]['TARGETID'])),return_indices=True)[:2]\n",
    "\n",
    "    all_repeat_TID=np.unique(np.concatenate((sv1_repeat_TID,sv2_repeat_TID,sv3_repeat_TID)))\n",
    "\n",
    "    sv1_only_mask=np.ones(len(MgII_pure[sv1_mask]),bool)\n",
    "    sv1_only_mask[sv1_repeat_idx] = 0\n",
    "    sv1_only=MgII_pure[sv1_mask][sv1_only_mask]\n",
    "\n",
    "    sv2_only_mask=np.ones(len(MgII_pure[sv2_mask]),bool)\n",
    "    sv2_only_mask[sv2_repeat_idx] = 0\n",
    "    sv2_only=MgII_pure[sv2_mask][sv2_only_mask]\n",
    "\n",
    "    sv3_only_mask=np.ones(len(MgII_pure[sv3_mask]),bool)\n",
    "    sv3_only_mask[sv3_repeat_idx] = 0\n",
    "    sv3_only=MgII_pure[sv3_mask][sv3_only_mask]\n",
    "\n",
    "    #First find the indices in the fujilupe catalog of the repeatping targetids\n",
    "    fujilupe_primary_repeat_idx=np.intersect1d(fujilupe[is_primary]['TARGETID'],all_repeat_TID,return_indices=True)[1]\n",
    "    #next take a subset of fujilupe corresponding tp those indices\n",
    "    fujilupe_primary_repeat=fujilupe[is_primary][fujilupe_primary_repeat_idx]\n",
    "\n",
    "    #Now lets determine which survey provides the best observations for this subset\n",
    "    sv1_best_mask=fujilupe_primary_repeat['SURVEY']=='sv1'\n",
    "    sv2_best_mask=fujilupe_primary_repeat['SURVEY']=='sv2'\n",
    "    sv3_best_mask=fujilupe_primary_repeat['SURVEY']=='sv3'\n",
    "\n",
    "    #Lets grab the subsets of fujilupe for which a certain targetid is best\n",
    "    sv1_best=fujilupe_primary_repeat[sv1_best_mask]\n",
    "    sv2_best=fujilupe_primary_repeat[sv2_best_mask]\n",
    "    sv3_best=fujilupe_primary_repeat[sv3_best_mask]\n",
    "\n",
    "    sv1_best_TID_idx=np.intersect1d(MgII_pure[sv1_mask]['TARGETID'],sv1_best['TARGETID'],return_indices=True)[1]\n",
    "    sv2_best_TID_idx=np.intersect1d(MgII_pure[sv2_mask]['TARGETID'],sv2_best['TARGETID'],return_indices=True)[1]\n",
    "    sv3_best_TID_idx=np.intersect1d(MgII_pure[sv3_mask]['TARGETID'],sv3_best['TARGETID'],return_indices=True)[1]\n",
    "\n",
    "    sv1_best=MgII_pure[sv1_mask][sv1_best_TID_idx]\n",
    "    sv2_best=MgII_pure[sv2_mask][sv2_best_TID_idx]\n",
    "    sv3_best=MgII_pure[sv3_mask][sv3_best_TID_idx]\n",
    "\n",
    "    print(len(sv1_only),len(sv2_only),len(sv3_only),len(sv1_best),len(sv2_best),len(sv3_best))\n",
    "\n",
    "    combined_pure_MgIIcat=np.concatenate([sv1_only,sv2_only,sv3_only,sv1_best,sv2_best,sv3_best],dtype=sv1_only.dtype)\n",
    "    print(len(combined_pure_MgIIcat))\n",
    "    fitsio.write(out_fn,combined_pure_MgIIcat,extname='MGII_ABSORBERS')\n",
    "\n",
    "if(redux == 'guadalupe'):\n",
    "    fitsio.write(out_fn,MgII_pure,extname='MGII_ABSORBERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb77603-c800-4d50-ab87-71a1b116bcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI developer",
   "language": "python",
   "name": "desi-developer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
