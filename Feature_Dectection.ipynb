{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import desispec.io\n",
    "\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "#from astropy.io.misc.hdf5 import read_table_hdf5\n",
    "from astropy import modeling\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "#using the commissioning targeting mask\n",
    "from desitarget.cmx.cmx_targetmask import cmx_mask  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn 'b', 'r', 'z' into 'brz'\n",
    "def quick_brz(specobj,spectrum):\n",
    "    #pull wavelength sections\n",
    "    bw=np.round(specobj.wave['b'],3)\n",
    "    rw=np.round(specobj.wave['r'],3)\n",
    "    zw=np.round(specobj.wave['z'],3)\n",
    "    \n",
    "    #find overlapping arrays in wave\n",
    "    br_overlap=np.intersect1d(bw,rw)\n",
    "    rz_overlap=np.intersect1d(rw,zw)\n",
    "\n",
    "    #find indices of overlapping regions\n",
    "    br_start=int(np.where(bw==br_overlap[0])[0])\n",
    "    br_end=int(len(bw))\n",
    "\n",
    "    rz_start=int(np.where(rw==rz_overlap[0])[0])\n",
    "    rz_end=int(len(rw))\n",
    "\n",
    "    #pull flux\n",
    "    bf=specobj.flux['b'][spectrum]\n",
    "    rf=specobj.flux['r'][spectrum]\n",
    "    zf=specobj.flux['z'][spectrum]\n",
    "    #pull error\n",
    "    be=1/np.sqrt(specobj.ivar['b'][spectrum])\n",
    "    re=1/np.sqrt(specobj.ivar['r'][spectrum])\n",
    "    ze=1/np.sqrt(specobj.ivar['z'][spectrum])\n",
    "    #turn into 'brz'\n",
    "    x_spc=np.concatenate((bw[:br_start],(bw[br_start:br_end]+rw[:br_end-br_start])/2,rw[br_end-br_start:rz_start],(rw[rz_start:rz_end]+zw[:rz_end-rz_start])/2,zw[rz_end-rz_start:]))\n",
    "    y_flx=np.concatenate((bf[:br_start],(bf[br_start:br_end]+rf[:br_end-br_start])/2,rf[br_end-br_start:rz_start],(rf[rz_start:rz_end]+zf[:rz_end-rz_start])/2,zf[rz_end-rz_start:]))\n",
    "    y_err=np.concatenate((be[:br_start],(be[br_start:br_end]+re[:br_end-br_start])/2,re[br_end-br_start:rz_start],(re[rz_start:rz_end]+ze[:rz_end-rz_start])/2,ze[rz_end-rz_start:]))\n",
    "    \n",
    "    return(x_spc,y_flx,y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Peakfinder function.\n",
    "Inputs: data - array of values in which to find peaks\n",
    "        err  - array of associated error values to data\n",
    "        threshold - level of standard deviation required to be defined as a peak\n",
    "Description:\n",
    "        Detects peaks in a 1d array with associated errors. First determines mean and standard deviation (std)\n",
    "        in 2000 index window around each index, then checks if the associated value is greater than mean+err+threshold*std.\n",
    "        If the value is then the value and its location in the array are appended to the output peaks array.\n",
    "Outputs:\n",
    "        peaks - 1d array holding locations of detected peaks\n",
    "\n",
    "''';\n",
    "def find_p(data,threshold,edge_clip=30,window_size=1000):\n",
    "    #initialize output array\n",
    "    peaks=[]\n",
    "    \n",
    "    #pad data array with 1 values (data is currently pased as a ratio of fit/signal)\n",
    "    data=(np.pad(data[edge_clip:-edge_clip],window_size,mode='constant',constant_values=1))\n",
    "    \n",
    "    #iterate over array length\n",
    "    for i in range(1000,len(data)):\n",
    "        #find windowed median and std\n",
    "        median=np.median(data[i-window_size:i+window_size])\n",
    "        #std=np.std(data[i-window_size:i+window_size])\n",
    "        #remove massive outliers (points where flux gets right near 0)\n",
    "        #these points are still considered for peaks, but not factored into std\n",
    "        std=np.std(data[i-window_size:i+window_size]<np.std(data)*20)\n",
    "            \n",
    "        #check if peak conditions are true\n",
    "        if(data[i]>median+threshold*std and data[i]==np.max(data[i-10:i+10])):\n",
    "            #append true location of peak pre padding\n",
    "            peaks.append(i-window_size+edge_clip)\n",
    "            #append value of peak\n",
    "            \n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from itertools import *\n",
    "fitter = modeling.fitting.LevMarLSQFitter()\n",
    "model = modeling.models.Gaussian1D()\n",
    "\n",
    "#whole thing can probably use a rewrite for the sake of efficency\n",
    "def doublet_finder(targetid,continuum,data,y_err,x_spc):\n",
    "    rf_line_sep=7.1755\n",
    "    #may want to adjust this down if too many matches\n",
    "    #I think this is what does most of the sample cutting (actually maybe not)\n",
    "    rf_err_margain=0.25\n",
    "    \n",
    "    residual=continuum-data\n",
    "    peaks=np.asarray(find_p(continuum/data,5.0),dtype=int)\n",
    "    \n",
    "    #Generate groups of data with positive residuals\n",
    "    #From https://stackoverflow.com/questions/3149440/python-splitting-list-based-on-missing-numbers-in-a-sequence\n",
    "    groups = []\n",
    "    for k, g in groupby(enumerate(np.where(residual>0)[0]), lambda x: x[0]-x[1]):\n",
    "        groups.append(list(map(itemgetter(1), g)))\n",
    "    \n",
    "    absorb_lines=[]\n",
    "    for group in groups:\n",
    "        #Skip groups of 1 or 2 data vals, these aren't worthwhile peaks and cause an issue in fitting\n",
    "        if(len(group) < 3):\n",
    "            continue\n",
    "        #calculate snr, seperate class for high signal\n",
    "        if(np.average(group)<5):\n",
    "            snr=np.sum(residual[group]/continuum[group])/np.sqrt(np.sum(y_err[group]**2))\n",
    "        else:\n",
    "            snr=np.sum(residual[group]/1)/np.sqrt(np.sum(y_err[group]**2))\n",
    "        #print(group,snr)\n",
    "        #this 3 threshold is mutable, experiment around I guess?\n",
    "        if(snr>3.5):\n",
    "            #Fit gaussian model\n",
    "            model = modeling.models.Gaussian1D(amplitude=np.nanmax(residual[group]),mean=np.average(x_spc[group]))\n",
    "            fm = fitter(model=model, x=x_spc[group], y=residual[group])\n",
    "            #determine redshift by model params\n",
    "            z=fm.parameters[1]/2795.5301-1\n",
    "\n",
    "            absorb_lines.append([z,group,snr,fm.parameters[0],fm.parameters[2]])\n",
    "            \n",
    "    peak_lines=[]\n",
    "    for a in absorb_lines:\n",
    "        for p in peaks:\n",
    "            if(p in a[1]):\n",
    "                peak_lines.append(a+[p])\n",
    "                    \n",
    "    doublets=[]\n",
    "    for line1 in peak_lines:\n",
    "        for line2 in peak_lines:\n",
    "            line_sep=rf_line_sep*(1+line1[0])\n",
    "            err_margain=rf_err_margain*(1+line1[0])\n",
    "            \n",
    "            low_zbound=(line_sep-err_margain)/2795.5301\n",
    "            high_zbound=(line_sep+err_margain)/2795.5301\n",
    "            #Check if z_seperation to second line is in appropriate range.\n",
    "            if(low_zbound<line2[0]-line1[0]<high_zbound):\n",
    "                #pass along the redshift of first line, redshift diff to second line, and both SNRs\n",
    "                doublets.append([str(targetid),line1[0],line1[2],line2[2],line1[3],line1[4],line2[3],line2[4]])\n",
    "    return doublets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "cnt=0\n",
    "#good_candidates = np.genfromtxt('Spec_w_goodCandidates.txt',dtype=str)\n",
    "#run over all spectrographs\n",
    "for s in range(0,10):\n",
    "    tile, date, spectrograph = \"68002\", \"20200315\", str(s)\n",
    "    #can adapt to forming dir name more cleanly\n",
    "    dirname = \"/global/cfs/cdirs/desi/spectro/redux/andes/tiles/68002/20200315\"\n",
    "    #dirname = os.path.join(os.getenv(\"DESI_SPECTRO_REDUX\"), os.getenv(\"SPECPROD\"), \"tiles\", tile, date)\n",
    "    filename = \"coadd-{}-{}-{}.fits\".format(spectrograph, tile, date)\n",
    "    \n",
    "    #open coadd spectrum and zbest files, will want to ope h5 file at some point\n",
    "    specfile = os.path.join(dirname, filename)\n",
    "    zbestfile=specfile.replace('coadd', 'zbest')\n",
    "    print(specfile)\n",
    "    #read them in using std method\n",
    "    specobj = desispec.io.read_spectra(specfile)\n",
    "    zbest = Table.read(zbestfile, hdu=1)\n",
    "    #grab targetids, ra, dec and z\n",
    "    fm = specobj.target_ids()\n",
    "    ra = specobj.fibermap['TARGET_RA']\n",
    "    dec =specobj.fibermap['TARGET_DEC']\n",
    "    redshifts=zbest['Z']\n",
    "    #extract all sources that were targeted as SV-like quasars\n",
    "    qsos = np.where(specobj.fibermap[\"CMX_TARGET\"] & cmx_mask[\"SV0_QSO\"])[0]\n",
    "    print(qsos)\n",
    "    #run over all spectra\n",
    "    for i in qsos:\n",
    "        spectrum = i\n",
    "        targetid=fm[i]\n",
    "\n",
    "        #define x_range and flux_vals\n",
    "        #see if \"brz\" is given\n",
    "        if(\"brz\" in specobj.wave):\n",
    "            x_spc = specobj.wave[\"brz\"]\n",
    "            y_flx = specobj.flux[\"brz\"][spectrum]\n",
    "            y_err=1/np.sqrt(specobj.ivar[\"brz\"][spectrum])\n",
    "        #else combine into \"brz\" using helper fnc.\n",
    "        else:\n",
    "            x_spc,y_flx,y_err=quick_brz(specobj,spectrum)\n",
    "            \n",
    "        #estimate continuum using median filter\n",
    "        cont_est = medfilt(y_flx,19)\n",
    "        #calculate residual\n",
    "        bc_residual = cont_est-y_flx\n",
    "\n",
    "        doublets=doublet_finder(str(targetid),cont_est,y_flx,y_err,x_spc)\n",
    "\n",
    "        for i in doublets:\n",
    "            results.append(i)\n",
    "np.savetxt('MgII_Doublets.csv', results, delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI developer",
   "language": "python",
   "name": "desi-developer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
